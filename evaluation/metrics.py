"""
Module de Calcul des MÃ©triques d'Ã‰valuation
Fonctions pour Ã©valuer les performances des modÃ¨les de segmentation
"""

import numpy as np
import tensorflow as tf
from sklearn.metrics import confusion_matrix, classification_report
from typing import Dict, List, Tuple
import json

# Import de la configuration
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from utils.config import *
from preprocessing.data_loader import RDD2022DataLoader
from preprocessing.preprocessing import ImagePreprocessor


def is_yolo_model(model) -> bool:
    """
    VÃ©rifie si le modÃ¨le est un modÃ¨le YOLO (Ultralytics)

    Args:
        model: Le modÃ¨le Ã  vÃ©rifier

    Returns:
        True si c'est un modÃ¨le YOLO, False sinon
    """
    model_type = str(type(model))
    return 'ultralytics' in model_type.lower() or 'yolo' in model_type.lower()


def predict_with_yolo(model, batch_images: np.ndarray, preprocessor: ImagePreprocessor, debug: bool = False) -> np.ndarray:
    """
    Effectue des prÃ©dictions avec un modÃ¨le YOLO

    YOLO nÃ©cessite un traitement spÃ©cial:
    - Les images doivent Ãªtre en uint8 (0-255), pas normalisÃ©es
    - Les prÃ©dictions retournent des objets Results qu'il faut convertir

    Args:
        model: ModÃ¨le YOLO (Ultralytics)
        batch_images: Batch d'images normalisÃ©es (N, H, W, C) en float32
        preprocessor: PrÃ©processeur pour dÃ©normaliser les images
        debug: Si True, affiche des informations de debug

    Returns:
        Masques prÃ©dits (N, H, W) avec class IDs
    """
    from models.yolo_pretrained import YOLOSegmentation
    import cv2

    predictions = []
    yolo_wrapper = YOLOSegmentation()

    # Compteur pour le debug
    total_detections = 0

    for idx, img in enumerate(batch_images):
        # DÃ©normaliser l'image pour YOLO (0-1 -> 0-255)
        img_uint8 = preprocessor.denormalize_image(img)

        # Redimensionner Ã  la taille attendue par YOLO (640x640)
        img_yolo = cv2.resize(img_uint8, (YOLO_CONFIG['img_size'], YOLO_CONFIG['img_size']))

        # PrÃ©dire avec YOLO - utiliser un seuil de confiance plus bas
        results = model.predict(
            source=img_yolo,
            conf=0.1,  # Seuil de confiance plus bas pour dÃ©tecter plus d'objets
            iou=YOLO_CONFIG['iou_threshold'],
            imgsz=YOLO_CONFIG['img_size'],
            save=False,
            verbose=False
        )

        # Convertir les masques YOLO en format standard
        mask = yolo_wrapper.convert_masks_to_segmentation(results, target_size=IMG_SIZE)

        # Debug: compter les dÃ©tections
        if results[0].masks is not None:
            num_detections = len(results[0].masks)
            total_detections += num_detections
            if debug and idx == 0:
                print(f"     - YOLO dÃ©tections image 0: {num_detections}")
                print(f"     - Classes dÃ©tectÃ©es: {results[0].boxes.cls.cpu().numpy() if results[0].boxes is not None else 'aucune'}")

        predictions.append(mask)

    if debug:
        print(f"     - Total dÃ©tections YOLO sur batch: {total_detections}")

    return np.array(predictions)


# ============================================================================
# Ã‰VALUATION GLOBALE DU MODÃˆLE
# ============================================================================

def evaluate_model_on_dataset(model,
                              data_loader: RDD2022DataLoader,
                              preprocessor: ImagePreprocessor,
                              num_samples: int = None,
                              batch_size: int = 8,
                              model_name: str = None) -> Dict:
    """
    Ã‰value un modÃ¨le sur un dataset complet

    Cette fonction prend un modÃ¨le entraÃ®nÃ© et calcule toutes les mÃ©triques
    importantes sur l'ensemble du dataset de test. Elle retourne un dictionnaire
    contenant les performances globales et par classe.

    Args:
        model: ModÃ¨le Keras ou YOLO Ã  Ã©valuer
        data_loader: Loader du dataset (train/val/test)
        preprocessor: PrÃ©processeur pour normaliser les images
        num_samples: Nombre d'Ã©chantillons Ã  Ã©valuer (None = tout le dataset)
        batch_size: Taille des batches pour l'Ã©valuation
        model_name: Nom du modÃ¨le (pour l'affichage)

    Returns:
        Dict contenant toutes les mÃ©triques :
        {
            'global': {
                'iou': float,
                'dice': float,
                'pixel_accuracy': float
            },
            'per_class': {
                class_id: {'iou': float, 'dice': float, 'precision': float, 'recall': float, 'f1': float}
            },
            'confusion_matrix': ndarray,
            'num_samples': int
        }
    """
    display_name = model_name if model_name else "modÃ¨le"
    print(f"\nðŸ“Š Ã‰valuation de {display_name} sur {data_loader.split}...")
    print("-" * 80)
    
    # DÃ©terminer le nombre d'Ã©chantillons
    if num_samples is None:
        num_samples = len(data_loader)
    else:
        num_samples = min(num_samples, len(data_loader))
    
    # Initialiser les accumulateurs pour les mÃ©triques
    all_y_true = []
    all_y_pred = []
    
    total_iou = 0
    total_dice = 0
    total_pixel_acc = 0
    
    # Ã‰valuer batch par batch
    num_batches = (num_samples + batch_size - 1) // batch_size

    # Debug: afficher les informations du premier batch
    debug_printed = False

    for batch_idx in range(num_batches):
        # Calculer les indices de dÃ©but et fin pour ce batch
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, num_samples)

        # Charger et prÃ©traiter les images du batch
        batch_images = []
        batch_masks_true = []

        for idx in range(start_idx, end_idx):
            image, mask, _ = data_loader[idx]

            # PrÃ©traiter
            proc_image = preprocessor.preprocess_image(image)
            proc_mask = preprocessor.preprocess_mask(mask)

            batch_images.append(proc_image)
            batch_masks_true.append(proc_mask)

        # Convertir en arrays
        batch_images = np.array(batch_images)
        batch_masks_true = np.array(batch_masks_true)

        # PrÃ©diction - gÃ©rer diffÃ©remment selon le type de modÃ¨le
        if is_yolo_model(model):
            # YOLO nÃ©cessite un traitement spÃ©cial
            # Activer le debug pour le premier batch
            batch_masks_pred = predict_with_yolo(model, batch_images, preprocessor, debug=not debug_printed)
            batch_predictions = None  # YOLO retourne directement les masques
        else:
            # ModÃ¨les Keras (U-Net, Hybrid)
            batch_predictions = model.predict(batch_images, verbose=False)
            # Convertir les prÃ©dictions en masques de classes
            batch_masks_pred = np.argmax(batch_predictions, axis=-1)

        # Debug: afficher les informations du premier batch pour diagnostiquer
        if not debug_printed:
            print(f"\n  ðŸ” DEBUG - Premier batch:")
            print(f"     - Shape images: {batch_images.shape}")
            print(f"     - Shape masques vrais: {batch_masks_true.shape}")
            if batch_predictions is not None:
                print(f"     - Shape prÃ©dictions brutes: {batch_predictions.shape}")
                print(f"     - Min/Max prÃ©dictions brutes: {batch_predictions.min():.4f} / {batch_predictions.max():.4f}")
            else:
                print(f"     - ModÃ¨le YOLO: prÃ©dictions converties directement en masques")
            print(f"     - Shape masques prÃ©dits: {batch_masks_pred.shape}")
            print(f"     - Classes uniques dans masques vrais: {np.unique(batch_masks_true)}")
            print(f"     - Classes uniques dans prÃ©dictions: {np.unique(batch_masks_pred)}")
            debug_printed = True
        
        # Accumuler pour la matrice de confusion globale
        all_y_true.extend(batch_masks_true.flatten())
        all_y_pred.extend(batch_masks_pred.flatten())
        
        # Calculer les mÃ©triques pour ce batch
        for i in range(len(batch_images)):
            # IoU pour cette image
            iou = calculate_iou_single(batch_masks_true[i], batch_masks_pred[i])
            total_iou += iou
            
            # Dice pour cette image
            dice = calculate_dice_single(batch_masks_true[i], batch_masks_pred[i])
            total_dice += dice
            
            # Pixel accuracy
            pixel_acc = calculate_pixel_accuracy_single(batch_masks_true[i], batch_masks_pred[i])
            total_pixel_acc += pixel_acc
        
        # Afficher la progression
        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == num_batches:
            print(f"  Batch {batch_idx + 1}/{num_batches} traitÃ©...")
    
    # Calculer les moyennes globales
    avg_iou = total_iou / num_samples
    avg_dice = total_dice / num_samples
    avg_pixel_acc = total_pixel_acc / num_samples
    
    print(f"\nâœ… Ã‰valuation terminÃ©e sur {num_samples} images")
    print(f"  - IoU moyen: {avg_iou:.4f}")
    print(f"  - Dice moyen: {avg_dice:.4f}")
    print(f"  - Pixel Accuracy: {avg_pixel_acc:.4f}")
    
    # Calculer la matrice de confusion
    conf_matrix = calculate_confusion_matrix(all_y_true, all_y_pred)
    
    # Calculer les mÃ©triques par classe
    per_class_metrics = calculate_class_metrics(all_y_true, all_y_pred)
    
    # Construire le dictionnaire de rÃ©sultats
    results = {
        'global': {
            'iou': float(avg_iou),
            'dice': float(avg_dice),
            'pixel_accuracy': float(avg_pixel_acc)
        },
        'per_class': per_class_metrics,
        'confusion_matrix': conf_matrix.tolist(),
        'num_samples': num_samples
    }
    
    return results


# ============================================================================
# CALCUL DES MÃ‰TRIQUES INDIVIDUELLES
# ============================================================================

def calculate_iou_single(mask_true: np.ndarray, mask_pred: np.ndarray) -> float:
    """
    Calcule le Mean IoU pour une seule paire de masques

    L'IoU (Intersection over Union) mesure le chevauchement entre deux masques.
    Plus l'IoU est Ã©levÃ© (proche de 1), meilleure est la prÃ©diction.

    Formule par classe: IoU = TP / (TP + FP + FN)
    Mean IoU = moyenne des IoU de chaque classe prÃ©sente

    Args:
        mask_true: Masque ground truth (H, W)
        mask_pred: Masque prÃ©dit (H, W)

    Returns:
        Mean IoU score entre 0 et 1
    """
    iou_per_class = []

    # Calculer l'IoU pour chaque classe prÃ©sente dans le ground truth ou la prÃ©diction
    for class_id in range(NUM_CLASSES + 1):
        # Masques binaires pour cette classe
        true_binary = (mask_true == class_id)
        pred_binary = (mask_pred == class_id)

        # Intersection et Union
        intersection = np.sum(true_binary & pred_binary)
        union = np.sum(true_binary | pred_binary)

        # Calculer IoU seulement si la classe existe dans ground truth ou prÃ©diction
        if union > 0:
            iou = intersection / union
            iou_per_class.append(iou)

    # Retourner la moyenne des IoU
    return np.mean(iou_per_class) if iou_per_class else 0.0


def calculate_dice_single(mask_true: np.ndarray, mask_pred: np.ndarray) -> float:
    """
    Calcule le Mean Dice coefficient pour une seule paire de masques

    Le coefficient de Dice est similaire Ã  l'IoU mais avec une formule diffÃ©rente.
    Il est particuliÃ¨rement utile pour les classes dÃ©sÃ©quilibrÃ©es.

    Formule par classe: Dice = 2 * |A âˆ© B| / (|A| + |B|)
    Mean Dice = moyenne des Dice de chaque classe prÃ©sente

    Args:
        mask_true: Masque ground truth (H, W)
        mask_pred: Masque prÃ©dit (H, W)

    Returns:
        Mean Dice coefficient entre 0 et 1
    """
    dice_per_class = []

    # Calculer le Dice pour chaque classe prÃ©sente
    for class_id in range(NUM_CLASSES + 1):
        # Masques binaires pour cette classe
        true_binary = (mask_true == class_id)
        pred_binary = (mask_pred == class_id)

        # Intersection et somme des cardinalitÃ©s
        intersection = np.sum(true_binary & pred_binary)
        sum_cardinality = np.sum(true_binary) + np.sum(pred_binary)

        # Calculer Dice seulement si la classe existe dans ground truth ou prÃ©diction
        if sum_cardinality > 0:
            dice = (2.0 * intersection) / sum_cardinality
            dice_per_class.append(dice)

    # Retourner la moyenne des Dice
    return np.mean(dice_per_class) if dice_per_class else 0.0


def calculate_pixel_accuracy_single(mask_true: np.ndarray, mask_pred: np.ndarray) -> float:
    """
    Calcule la prÃ©cision pixel par pixel
    
    C'est la mÃ©trique la plus simple : quel pourcentage de pixels
    a Ã©tÃ© correctement classifiÃ© ?
    
    Args:
        mask_true: Masque ground truth (H, W)
        mask_pred: Masque prÃ©dit (H, W)
        
    Returns:
        Accuracy entre 0 et 1
    """
    correct = np.sum(mask_true == mask_pred)
    total = mask_true.size
    
    return correct / total if total > 0 else 0


# ============================================================================
# MATRICE DE CONFUSION
# ============================================================================

def calculate_confusion_matrix(y_true: List, y_pred: List) -> np.ndarray:
    """
    Calcule la matrice de confusion pour la segmentation
    
    La matrice de confusion montre comment les pixels de chaque classe vraie
    ont Ã©tÃ© prÃ©dits. C'est trÃ¨s utile pour identifier les confusions entre classes.
    
    Par exemple, si le modÃ¨le confond souvent les fissures longitudinales (0)
    avec les fissures transversales (1), cela sera visible dans la matrice.
    
    Args:
        y_true: Liste des labels vrais (tous les pixels)
        y_pred: Liste des labels prÃ©dits (tous les pixels)
        
    Returns:
        Matrice de confusion (num_classes, num_classes)
    """
    # Convertir en arrays numpy
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    
    # Calculer la matrice de confusion
    conf_matrix = confusion_matrix(
        y_true, 
        y_pred, 
        labels=list(range(NUM_CLASSES + 1))  # 0, 1, 2, 3, 4
    )
    
    return conf_matrix


# ============================================================================
# MÃ‰TRIQUES PAR CLASSE
# ============================================================================

def calculate_class_metrics(y_true: List, y_pred: List) -> Dict:
    """
    Calcule les mÃ©triques dÃ©taillÃ©es pour chaque classe
    
    Pour chaque type de dommage routier, cette fonction calcule :
    - IoU : Chevauchement entre vrai et prÃ©dit
    - Dice : Coefficient de similaritÃ©
    - PrÃ©cision : Parmi les pixels prÃ©dits comme cette classe, combien sont corrects
    - Rappel : Parmi les vrais pixels de cette classe, combien ont Ã©tÃ© dÃ©tectÃ©s
    - F1-Score : Moyenne harmonique de prÃ©cision et rappel
    
    Args:
        y_true: Labels vrais
        y_pred: Labels prÃ©dits
        
    Returns:
        Dict {class_id: {'iou': X, 'dice': Y, 'precision': Z, ...}}
    """
    # Convertir en arrays
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    
    metrics = {}
    
    # Pour chaque classe (0, 1, 2, 4 + background)
    for class_id in range(NUM_CLASSES + 1):
        # CrÃ©er des masques binaires pour cette classe
        true_binary = (y_true == class_id).astype(int)
        pred_binary = (y_pred == class_id).astype(int)
        
        # Calculer TP, FP, FN, TN
        tp = np.sum((true_binary == 1) & (pred_binary == 1))  # True Positives
        fp = np.sum((true_binary == 0) & (pred_binary == 1))  # False Positives
        fn = np.sum((true_binary == 1) & (pred_binary == 0))  # False Negatives
        tn = np.sum((true_binary == 0) & (pred_binary == 0))  # True Negatives
        
        # Calculer IoU
        intersection = tp
        union = tp + fp + fn
        iou = intersection / union if union > 0 else 0
        
        # Calculer Dice
        dice = (2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0
        
        # Calculer PrÃ©cision
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        
        # Calculer Rappel
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        
        # Calculer F1-Score
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        # Stocker les mÃ©triques
        metrics[class_id] = {
            'iou': float(iou),
            'dice': float(dice),
            'precision': float(precision),
            'recall': float(recall),
            'f1': float(f1),
            'support': int(np.sum(true_binary))  # Nombre de pixels de cette classe
        }
    
    return metrics


def calculate_pixel_accuracy(y_true: List, y_pred: List) -> float:
    """
    Calcule la prÃ©cision globale pixel par pixel
    
    Args:
        y_true: Labels vrais
        y_pred: Labels prÃ©dits
        
    Returns:
        Accuracy globale
    """
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    
    correct = np.sum(y_true == y_pred)
    total = len(y_true)
    
    return correct / total if total > 0 else 0


def calculate_metrics_per_class(mask_true: np.ndarray, 
                                mask_pred: np.ndarray) -> Dict:
    """
    Version simplifiÃ©e pour une seule image
    
    Calcule les mÃ©triques par classe pour une paire de masques.
    Utile pour analyser les performances sur des images individuelles.
    
    Args:
        mask_true: Masque ground truth (H, W)
        mask_pred: Masque prÃ©dit (H, W)
        
    Returns:
        Dict {class_id: {'iou': X, 'dice': Y}}
    """
    metrics = {}
    
    for class_id in range(NUM_CLASSES + 1):
        # Masques binaires
        true_mask = (mask_true == class_id).astype(np.float32)
        pred_mask = (mask_pred == class_id).astype(np.float32)
        
        # Intersection et union
        intersection = np.sum(true_mask * pred_mask)
        union = np.sum(true_mask) + np.sum(pred_mask) - intersection
        
        # IoU
        iou = intersection / union if union > 0 else 0
        
        # Dice
        dice = (2 * intersection) / (np.sum(true_mask) + np.sum(pred_mask)) if (np.sum(true_mask) + np.sum(pred_mask)) > 0 else 0
        
        metrics[class_id] = {
            'iou': float(iou),
            'dice': float(dice)
        }
    
    return metrics


# ============================================================================
# FONCTION DE TEST
# ============================================================================

def test_metrics():
    """
    Fonction de test des mÃ©triques
    """
    print("\n" + "=" * 80)
    print("TEST DES MÃ‰TRIQUES D'Ã‰VALUATION")
    print("=" * 80)
    
    # CrÃ©er des masques de test
    mask_true = np.random.randint(0, NUM_CLASSES + 1, (256, 256))
    mask_pred = np.random.randint(0, NUM_CLASSES + 1, (256, 256))
    
    print(f"\nðŸ“Š Test sur masques alÃ©atoires:")
    print(f"  Shape: {mask_true.shape}")
    print(f"  Classes: {np.unique(mask_true)}")
    
    # Test IoU
    iou = calculate_iou_single(mask_true, mask_pred)
    print(f"\nâœ… IoU: {iou:.4f}")
    
    # Test Dice
    dice = calculate_dice_single(mask_true, mask_pred)
    print(f"âœ… Dice: {dice:.4f}")
    
    # Test Pixel Accuracy
    acc = calculate_pixel_accuracy_single(mask_true, mask_pred)
    print(f"âœ… Pixel Accuracy: {acc:.4f}")
    
    # Test mÃ©triques par classe
    per_class = calculate_metrics_per_class(mask_true, mask_pred)
    print(f"\nâœ… MÃ©triques par classe:")
    for class_id, metrics in per_class.items():
        print(f"  Classe {class_id}: IoU={metrics['iou']:.4f}, Dice={metrics['dice']:.4f}")
    
    print("\n" + "=" * 80)
    print("âœ… TOUS LES TESTS SONT PASSÃ‰S!")
    print("=" * 80)


if __name__ == "__main__":
    test_metrics()