"""
Script Principal d'√âvaluation
√âvalue et compare les 3 architectures : U-Net, YOLO, Hybrid
"""

import sys
import os

# Ajouter le r√©pertoire parent au path
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

import numpy as np
import json
from typing import Dict
from datetime import datetime

# Imports du projet
from preprocessing.data_loader import RDD2022DataLoader
from preprocessing.preprocessing import ImagePreprocessor
from evaluation.metrics import evaluate_model_on_dataset
from evaluation.visualization import create_evaluation_report, plot_predictions_grid
from utils.config import *
from utils.helpers import create_experiment_folder, save_results_json, print_summary_table

# Imports pour charger les mod√®les
import tensorflow as tf
from tensorflow import keras


# ============================================================================
# CHARGEMENT DES MOD√àLES
# ============================================================================

def load_trained_models(model_paths: Dict[str, str]) -> Dict:
    """
    Charge les 3 mod√®les entra√Æn√©s depuis le disque
    
    Cette fonction charge les mod√®les que tu as d√©j√† entra√Æn√©s.
    Elle g√®re les diff√©rents formats (Keras .h5 pour U-Net et Hybrid,
    et YOLO pour le mod√®le YOLO).
    
    Args:
        model_paths: Dict {model_name: path_to_model}
        Exemple:
        {
            'U-Net': 'results/models/unet_best.h5',
            'YOLO': 'results/models/yolo_best.pt',
            'Hybrid': 'results/models/hybrid_best.h5'
        }
        
    Returns:
        Dict {model_name: loaded_model}
    """
    print("\n" + "=" * 100)
    print("CHARGEMENT DES MOD√àLES ENTRA√éN√âS")
    print("=" * 100)
    
    models = {}
    
    for model_name, model_path in model_paths.items():
        print(f"\nüì¶ Chargement de {model_name}...")
        
        if not os.path.exists(model_path):
            print(f"   ‚ö†Ô∏è  Fichier non trouv√©: {model_path}")
            print(f"   ‚ö†Ô∏è  Ce mod√®le sera ignor√© dans l'√©valuation")
            continue
        
        try:
            if model_name == 'YOLO':
                # Charger le mod√®le YOLO (Ultralytics)
                try:
                    from ultralytics import YOLO
                    model = YOLO(model_path)
                    print(f"   ‚úÖ YOLO charg√© depuis: {model_path}")
                except ImportError:
                    print(f"   ‚ùå Ultralytics non disponible. Installez avec: pip install ultralytics")
                    continue
            else:
                # Charger les mod√®les Keras (U-Net et Hybrid)
                from models.model_utils import DiceCoefficient, IoUMetric
                
                # Custom objects pour les m√©triques personnalis√©es
                custom_objects = {
                    'DiceCoefficient': DiceCoefficient,
                    'IoUMetric': IoUMetric,
                    'dice_coefficient': DiceCoefficient,
                    'iou_metric': IoUMetric
                }
                
                model = keras.models.load_model(model_path, custom_objects=custom_objects)
                print(f"   ‚úÖ {model_name} charg√© depuis: {model_path}")
            
            models[model_name] = model
            
        except Exception as e:
            print(f"   ‚ùå Erreur lors du chargement de {model_name}: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"\n‚úÖ {len(models)} mod√®les charg√©s avec succ√®s")
    return models


# ============================================================================
# PR√âDICTION AVEC LES MOD√àLES
# ============================================================================

def predict_with_model(model, 
                      model_name: str,
                      images: np.ndarray,
                      preprocessor: ImagePreprocessor) -> np.ndarray:
    """
    Effectue des pr√©dictions avec un mod√®le donn√©
    
    Cette fonction g√®re les diff√©rences entre les mod√®les :
    - U-Net et Hybrid : pr√©diction directe avec Keras
    - YOLO : pr√©diction puis conversion du format YOLO vers notre format
    
    Args:
        model: Le mod√®le (Keras ou YOLO)
        model_name: Nom du mod√®le ('U-Net', 'YOLO', 'Hybrid')
        images: Batch d'images pr√©trait√©es (N, H, W, C)
        preprocessor: Pr√©processeur pour la normalisation
        
    Returns:
        Masques pr√©dits (N, H, W) avec class IDs
    """
    if model_name == 'YOLO':
        # Pr√©diction YOLO
        predictions = []
        
        for img in images:
            # D√©normaliser l'image pour YOLO
            img_uint8 = preprocessor.denormalize_image(img)
            
            # Pr√©dire
            results = model.predict(source=img_uint8, conf=0.25, save=False, verbose=False)
            
            # Convertir les masques YOLO en format standard
            from models.yolo_pretrained import YOLOSegmentation
            yolo_wrapper = YOLOSegmentation()
            mask = yolo_wrapper.convert_masks_to_segmentation(results, target_size=IMG_SIZE)
            
            predictions.append(mask)
        
        return np.array(predictions)
    
    else:
        # Pr√©diction Keras (U-Net et Hybrid)
        predictions = model.predict(images, verbose=0)
        
        # Convertir les probabilit√©s en classes
        masks = np.argmax(predictions, axis=-1)
        
        return masks


# ============================================================================
# √âVALUATION COMPL√àTE DES MOD√àLES
# ============================================================================

def evaluate_all_models(data_path: str,
                       model_paths: Dict[str, str],
                       num_samples: int = None,
                       batch_size: int = 8,
                       save_predictions: bool = True) -> Dict:
    """
    Fonction principale d'√©valuation
    
    Cette fonction orchestre toute l'√©valuation :
    1. Charge les mod√®les entra√Æn√©s
    2. Charge le dataset de test
    3. √âvalue chaque mod√®le
    4. Calcule toutes les m√©triques
    5. G√©n√®re tous les graphiques
    6. Sauvegarde les r√©sultats
    
    C'est la fonction que tu vas appeler pour lancer l'√©valuation compl√®te.
    
    Args:
        data_path: Chemin vers RDD_SPLIT
        model_paths: Dict {model_name: path_to_model}
        num_samples: Nombre d'√©chantillons √† √©valuer (None = tous)
        batch_size: Taille des batches
        save_predictions: Si True, sauvegarde des exemples de pr√©dictions
        
    Returns:
        Dict avec tous les r√©sultats
    """
    print("\n" + "üéØ " * 40)
    print("√âVALUATION COMPL√àTE DES MOD√àLES DE SEGMENTATION")
    print("üéØ " * 40)
    
    # ========================================================================
    # 1. PR√âPARATION
    # ========================================================================
    print("\nüì¶ PHASE 1: Pr√©paration")
    print("-" * 100)
    
    # Cr√©er un dossier pour cette √©valuation
    eval_folder = create_experiment_folder("evaluation")
    print(f"‚úÖ Dossier d'√©valuation cr√©√©: {eval_folder}")
    
    # Charger les mod√®les
    models = load_trained_models(model_paths)
    
    if len(models) == 0:
        print("\n‚ùå Aucun mod√®le n'a pu √™tre charg√©. V√©rifiez les chemins.")
        return {}
    
    # Charger le dataset de test
    print(f"\nüìÇ Chargement du dataset de test...")
    test_loader = RDD2022DataLoader(data_path, split='test')
    print(f"‚úÖ Dataset de test: {len(test_loader)} images")
    
    # Cr√©er le pr√©processeur
    preprocessor = ImagePreprocessor(target_size=IMG_SIZE, normalize=True)
    
    # ========================================================================
    # 2. √âVALUATION DE CHAQUE MOD√àLE
    # ========================================================================
    print("\nüìä PHASE 2: √âvaluation des mod√®les")
    print("-" * 100)
    
    results_dict = {}
    predictions_dict = {}
    
    for model_name, model in models.items():
        print(f"\n{'=' * 100}")
        print(f"√âVALUATION DE {model_name.upper()}")
        print(f"{'=' * 100}")
        
        # √âvaluer le mod√®le
        results = evaluate_model_on_dataset(
            model=model,
            data_loader=test_loader,
            preprocessor=preprocessor,
            num_samples=num_samples,
            batch_size=batch_size,
            model_name=model_name
        )
        
        results_dict[model_name] = results
        
        # Sauvegarder quelques pr√©dictions pour visualisation
        if save_predictions:
            print(f"\nüíæ G√©n√©ration de pr√©dictions pour visualisation...")
            sample_predictions = []
            
            for idx in range(min(10, len(test_loader))):
                image, mask, _ = test_loader[idx]
                proc_image = preprocessor.preprocess_image(image)
                
                # Pr√©dire
                pred_mask = predict_with_model(
                    model, 
                    model_name, 
                    np.array([proc_image]), 
                    preprocessor
                )[0]
                
                sample_predictions.append(pred_mask)
            
            predictions_dict[model_name] = sample_predictions
    
    # ========================================================================
    # 3. COMPARAISON ET VISUALISATION
    # ========================================================================
    print("\nüìä PHASE 3: G√©n√©ration des graphiques de comparaison")
    print("-" * 100)
    
    figures_dir = os.path.join(eval_folder, 'figures')
    os.makedirs(figures_dir, exist_ok=True)
    
    # Cr√©er tous les graphiques
    create_evaluation_report(results_dict, output_dir=figures_dir)
    
    # Cr√©er la grille de pr√©dictions si on a sauvegard√© les pr√©dictions
    if save_predictions and predictions_dict:
        print("\nüìä Cr√©ation de la grille de pr√©dictions...")
        
        # Charger quelques images de test
        sample_images = []
        sample_masks = []
        
        for idx in range(min(4, len(test_loader))):
            image, mask, _ = test_loader[idx]
            sample_images.append(image)
            sample_masks.append(preprocessor.preprocess_mask(mask))
        
        # Cr√©er la grille
        save_path = os.path.join(figures_dir, 'predictions_grid.png')
        plot_predictions_grid(
            images=sample_images,
            masks_true=sample_masks,
            predictions_dict=predictions_dict,
            num_samples=min(4, len(sample_images)),
            save_path=save_path
        )
    
    # ========================================================================
    # 4. SAUVEGARDE DES R√âSULTATS
    # ========================================================================
    print("\nüíæ PHASE 4: Sauvegarde des r√©sultats")
    print("-" * 100)
    
    # Sauvegarder en JSON
    results_file = os.path.join(eval_folder, 'evaluation_results.json')
    save_results_json(results_dict, results_file)
    
    # Cr√©er un r√©sum√© texte
    summary_file = os.path.join(eval_folder, 'evaluation_summary.txt')
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("=" * 100 + "\n")
        f.write("R√âSUM√â DE L'√âVALUATION DES MOD√àLES\n")
        f.write("=" * 100 + "\n\n")
        
        f.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Dataset: {data_path}\n")
        f.write(f"Nombre d'images √©valu√©es: {num_samples or len(test_loader)}\n\n")
        
        # Tableau comparatif
        f.write("PERFORMANCES GLOBALES\n")
        f.write("-" * 100 + "\n\n")
        
        # En-t√™te
        f.write(f"{'Mod√®le':<15} {'IoU':<10} {'Dice':<10} {'Pixel Accuracy':<15}\n")
        f.write("-" * 100 + "\n")
        
        # Lignes
        for model_name, results in results_dict.items():
            iou = results['global']['iou']
            dice = results['global']['dice']
            acc = results['global']['pixel_accuracy']
            f.write(f"{model_name:<15} {iou:<10.4f} {dice:<10.4f} {acc:<15.4f}\n")
        
        f.write("\n" + "=" * 100 + "\n")
        
        # M√©triques par classe
        f.write("\nPERFORMANCES PAR CLASSE\n")
        f.write("-" * 100 + "\n\n")
        
        for model_name, results in results_dict.items():
            f.write(f"\n{model_name}:\n")
            f.write("-" * 50 + "\n")
            
            for class_id, metrics in results['per_class'].items():
                class_name = CLASS_NAMES.get(class_id - 1 if class_id > 0 else 0, f"Classe {class_id}")
                if class_id == 0:
                    class_name = "Background"
                
                f.write(f"  {class_name:<20} IoU: {metrics['iou']:.4f}  Dice: {metrics['dice']:.4f}\n")
    
    print(f"‚úÖ R√©sultats sauvegard√©s: {results_file}")
    print(f"‚úÖ R√©sum√© sauvegard√©: {summary_file}")
    
    # ========================================================================
    # 5. AFFICHAGE DU TABLEAU R√âCAPITULATIF
    # ========================================================================
    print("\n" + "=" * 100)
    print("TABLEAU R√âCAPITULATIF DES PERFORMANCES")
    print("=" * 100)
    
    # Pr√©parer les donn√©es pour le tableau
    comparison_data = {}
    for model_name, results in results_dict.items():
        comparison_data[model_name] = {
            'IoU': results['global']['iou'],
            'Dice': results['global']['dice'],
            'Pixel Accuracy': results['global']['pixel_accuracy']
        }
    
    print_summary_table(comparison_data)
    
    # Identifier le meilleur mod√®le
    best_model_iou = max(results_dict.items(), key=lambda x: x[1]['global']['iou'])
    best_model_dice = max(results_dict.items(), key=lambda x: x[1]['global']['dice'])
    
    print("\nüèÜ MEILLEURS MOD√àLES:")
    print(f"  - Meilleur IoU: {best_model_iou[0]} ({best_model_iou[1]['global']['iou']:.4f})")
    print(f"  - Meilleur Dice: {best_model_dice[0]} ({best_model_dice[1]['global']['dice']:.4f})")
    
    print("\n" + "=" * 100)
    print("‚úÖ √âVALUATION COMPL√àTE TERMIN√âE !")
    print("=" * 100)
    
    print(f"\nüìÅ Tous les r√©sultats sont dans: {eval_folder}")
    print("\nüìä Graphiques cr√©√©s:")
    print("  - comparison_global.png")
    print("  - comparison_iou_per_class.png")
    print("  - comparison_dice_per_class.png")
    print("  - confusion_matrices.png")
    print("  - predictions_grid.png")
    
    return results_dict


# ============================================================================
# FONCTIONS UTILITAIRES
# ============================================================================

def compare_models(results_dict: Dict) -> str:
    """
    Compare les mod√®les et g√©n√®re un rapport textuel
    
    Cette fonction analyse les r√©sultats et g√©n√®re des insights
    sur les forces et faiblesses de chaque mod√®le.
    
    Args:
        results_dict: Dict avec les r√©sultats de tous les mod√®les
        
    Returns:
        String avec le rapport d'analyse
    """
    report = []
    report.append("\n" + "=" * 100)
    report.append("ANALYSE COMPARATIVE DES MOD√àLES")
    report.append("=" * 100 + "\n")
    
    # Comparaison globale
    report.append("1. PERFORMANCES GLOBALES")
    report.append("-" * 100)
    
    for model_name, results in results_dict.items():
        iou = results['global']['iou']
        dice = results['global']['dice']
        acc = results['global']['pixel_accuracy']
        
        report.append(f"\n{model_name}:")
        report.append(f"  - IoU: {iou:.4f}")
        report.append(f"  - Dice: {dice:.4f}")
        report.append(f"  - Accuracy: {acc:.4f}")
    
    # Analyse par classe
    report.append("\n\n2. ANALYSE PAR TYPE DE DOMMAGE")
    report.append("-" * 100)
    
    # Trouver les classes les plus difficiles
    class_difficulties = {cid: [] for cid in range(NUM_CLASSES + 1)}
    
    for model_name, results in results_dict.items():
        for class_id, metrics in results['per_class'].items():
            class_difficulties[class_id].append((model_name, metrics['iou']))
    
    for class_id, performances in class_difficulties.items():
        if class_id == 0:
            class_name = "Background"
        else:
            class_name = CLASS_NAMES.get(class_id - 1, f"Classe {class_id}")
        
        avg_iou = np.mean([p[1] for p in performances])
        best_model = max(performances, key=lambda x: x[1])
        
        report.append(f"\n{class_name}:")
        report.append(f"  - IoU moyen: {avg_iou:.4f}")
        report.append(f"  - Meilleur mod√®le: {best_model[0]} (IoU: {best_model[1]:.4f})")
        
        if avg_iou < 0.5:
            report.append(f"  ‚ö†Ô∏è  Classe difficile (IoU < 0.5)")
    
    report.append("\n" + "=" * 100)
    
    return "\n".join(report)


def save_evaluation_results(results_dict: Dict, output_path: str):
    """
    Sauvegarde les r√©sultats d'√©valuation en JSON
    
    Args:
        results_dict: R√©sultats de l'√©valuation
        output_path: Chemin de sauvegarde
    """
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results_dict, f, indent=4, ensure_ascii=False)
    
    print(f"‚úÖ R√©sultats sauvegard√©s: {output_path}")


# ============================================================================
# SCRIPT PRINCIPAL
# ============================================================================

if __name__ == "__main__":
    """
    Script principal d'√©valuation
    
    Pour utiliser ce script :
    1. Assure-toi que tes mod√®les sont entra√Æn√©s et sauvegard√©s
    2. Modifie les chemins vers tes mod√®les ci-dessous
    3. Lance le script : python evaluation/evaluate.py
    """
    
    # Configuration
    DATA_PATH = "C:/Users/DELL/Desktop/dataset/RDD_SPLIT"
    
    # Chemins vers les mod√®les entra√Æn√©s
    # ‚ö†Ô∏è MODIFIE CES CHEMINS SELON TES MOD√àLES ENTRA√éN√âS
    MODEL_PATHS = {
        'U-Net': 'C:/Users/DELL/Desktop/results/models/unet_100img_20260103_231900.keras',
        'YOLO': 'C:/Users/DELL/Desktop/dataset/yolo_temp_results/yolo_100img2/weights/last.pt',  # Chemin YOLO typique
        'Hybrid': 'C:/Users/DELL/Desktop/results/models/hybrid_100img_20260104_111932.keras'
    }
    
    # Param√®tres d'√©valuation
    NUM_SAMPLES = 100  # None = tout le dataset de test
    BATCH_SIZE = 8
    
    # Lancer l'√©valuation compl√®te
    results = evaluate_all_models(
        data_path=DATA_PATH,
        model_paths=MODEL_PATHS,
        num_samples=NUM_SAMPLES,
        batch_size=BATCH_SIZE,
        save_predictions=True
    )
    
    # Afficher l'analyse comparative
    if results:
        analysis = compare_models(results)
        print(analysis)
